{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from metric import get_mrr, get_recall, get_ndcg\n",
    "import datetime\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from Model.PairRNN import PairSelfAttentionLayer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def Construct_connection_matrix(seqs, seq_index, seq_len, actions):\n",
    "    # mask = seqs.gt(0)\n",
    "    items, n_node, A, alias_inputs, seq_real_len, macro_items, micro_actions, micro_actions_len = [], [], [], [], [], [], [], []\n",
    "    num_seqs = seqs.size(0)\n",
    "    # max_n_node = seqs.size(1)\n",
    "    item_seq = seqs.cpu().numpy()\n",
    "    for i in range(num_seqs):\n",
    "        u_input = item_seq[i]\n",
    "        u_index = seq_index[i]\n",
    "        u_seq_len = seq_len[i]\n",
    "        u_actions = actions[i]\n",
    "        u_index = u_index[:u_seq_len.long()] \n",
    "        # seq_index[-1] = seq_index[-2] + 1\n",
    "        u_macro_items = torch.index_select(seqs[i], dim=0, index=u_index.long()) \n",
    "        u_macro_items = u_macro_items.numpy()\n",
    "        node = np.unique(u_macro_items) #\n",
    "        u_A_length = len(node) \n",
    "        u_A_edge = len(u_macro_items) \n",
    "        items.append(torch.LongTensor(node)) \n",
    "        u_A_in = np.zeros((u_A_length, u_A_edge)) \n",
    "        u_A_out = np.zeros((u_A_length, u_A_edge))\n",
    "        for i in np.arange(len(u_macro_items) - 1): \n",
    "            if u_macro_items[i] == 0:\n",
    "                continue\n",
    "            u = np.where(node == u_macro_items[i])[0][0]\n",
    "            v = np.where(node == u_macro_items[i + 1])[0][0]\n",
    "            u_A_in[v][i] = 1 \n",
    "            u_A_out[u][i] = 1 \n",
    "        macro_items.append(torch.LongTensor(u_macro_items))\n",
    "        u_A = np.concatenate([u_A_in, u_A_out])\n",
    "        A.append(torch.LongTensor(u_A))\n",
    "        alias_input = []\n",
    "        for i in u_input:\n",
    "            if i == 0:\n",
    "                continue\n",
    "            else:\n",
    "                alias_input.append(np.where(node == i)[0][0]) \n",
    "        seq_real_len.append(len(alias_input))\n",
    "        alias_inputs.append(torch.LongTensor(alias_input))\n",
    "        u_micro_actions, u_micro_actions_len = seq_action_to_micro_action(u_actions, u_index.long())\n",
    "        micro_actions.append(u_micro_actions)\n",
    "        micro_actions_len.append(u_micro_actions_len)\n",
    "    # B, n_items; B, n_seq; B, n_items, n_edges; B, 1; B, n_edges; B, n_edges, n_micro_actions; B, n_edges, 1\n",
    "    return items, alias_inputs, A, seq_real_len, macro_items, micro_actions, micro_actions_len\n",
    "\n",
    "def seq_action_to_micro_action(actions, seq_index):\n",
    "    seq_len = actions.size(0)\n",
    "    mask = actions.gt(0).float()\n",
    "    real_length = torch.sum(mask).long()\n",
    "    padding_len = seq_len - real_length\n",
    "    last_index = padding_len\n",
    "    micro_actions = []\n",
    "    micro_actions_len = []\n",
    "    for action_index in seq_index:\n",
    "        micro_action = actions[last_index:action_index+1]\n",
    "        last_index = action_index + 1\n",
    "        micro_actions.append(torch.LongTensor(micro_action.long()))\n",
    "        micro_actions_len.append(len(micro_action))\n",
    "    micro_actions_len = torch.LongTensor(micro_actions_len)\n",
    "    # max_len = 39\n",
    "    # last_micro_action = micro_actions.pop(-1)\n",
    "    # last_padding_action = torch.zeros(max_len)\n",
    "    # last_padding_action[:last_micro_action.size(0)] = last_micro_action\n",
    "    # micro_actions.append(last_padding_action)\n",
    "    return micro_actions, micro_actions_len\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class ListDataset(Dataset):\n",
    "    def __init__(self, *datalist):\n",
    "        assert all(len(datalist[0]) == len(data) for data in datalist)\n",
    "        self.datalist = datalist\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return tuple(data[index] for data in self.datalist)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datalist[0])\n",
    "    \n",
    "def batch_padding(batch):\n",
    "    # 根据信息进行padding\n",
    "    # print(len(batch[0]))\n",
    "    item_ids, alias_inputs, A, seq_real_len, macro_items, micro_actions, micro_len, action, pairs, poses, y = zip(*batch)\n",
    "    item_ids = pad_sequence(item_ids, batch_first=True).long()\n",
    "    alias_inputs = pad_sequence(alias_inputs, batch_first=True, padding_value=-1).long()\n",
    "    max_action_len = alias_inputs.size(1)\n",
    "    batch_max_length = item_ids.size(1)\n",
    "    macro_items = pad_sequence(macro_items, batch_first=True, padding_value=0) # n_edges\n",
    "    batch_edge_length = macro_items.size(1)\n",
    "    new_A = []\n",
    "    for a in A:\n",
    "        node_len, edge_len = int(a.size(0)/2), a.size(1)\n",
    "        a_in, a_out = a[:node_len, :], a[node_len:, :]\n",
    "        pad_items_length = batch_max_length - node_len\n",
    "        pad_edges_length = batch_edge_length - edge_len\n",
    "        pad_tuple = (0, pad_edges_length, 0, pad_items_length)\n",
    "        a_in, a_out = F.pad(a_in, pad_tuple), F.pad(a_out, pad_tuple)\n",
    "        new_A.append(torch.cat((a_in, a_out), 1).tolist())\n",
    "    new_micro_actions = []\n",
    "    for ac in micro_actions:\n",
    "        new_ac = pad_sequence(ac, batch_first=True, padding_value=0)\n",
    "        pad_edge_size = batch_edge_length - new_ac.size(0)\n",
    "        pad_action_size = max_action_len - new_ac.size(1)\n",
    "        new_ac = F.pad(new_ac, (0, pad_action_size, 0, pad_edge_size))\n",
    "        new_micro_actions.append(new_ac.tolist())\n",
    "    micro_len = pad_sequence(micro_len, batch_first=True, padding_value=1) # 这里需要注意，padding的内容最后是用不上的\n",
    "    action = pad_sequence(action, batch_first=True)\n",
    "    poses = pad_sequence(poses, batch_first=True)\n",
    "    new_pairs = []\n",
    "    seq_len = action.size(1)\n",
    "    for pair in pairs:\n",
    "        length = pair.size(0)\n",
    "        pad_length = seq_len - length\n",
    "        pad_tuple = (0, pad_length, 0, pad_length)\n",
    "        new_pair = F.pad(pair, pad_tuple)\n",
    "        new_pairs.append(new_pair.tolist())   \n",
    "    return item_ids, alias_inputs, torch.LongTensor(new_A), torch.LongTensor(seq_real_len),torch.LongTensor(macro_items), torch.LongTensor(new_micro_actions), torch.LongTensor(micro_len), action, torch.Tensor(new_pairs), poses, torch.LongTensor(y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_data(data_name):\n",
    "    if data_name == 'Trivago':\n",
    "        train_sets = torch.load('data/%s/train_sets_CasRNNGNN.pt'% data_name)\n",
    "        valid_sets = torch.load('data/%s/valid_sets_CasRNNGNN.pt'% data_name)\n",
    "        test_sets = torch.load('data/%s/test_sets_CasRNNGNN.pt'% data_name)\n",
    "    else:\n",
    "        train_dataload = torch.load( 'data/%s/train_dataload_CasRNNGNN.pt'% data_name)\n",
    "        valid_dataload = torch.load( 'data/%s/valid_dataload_CasRNNGNN.pt'% data_name)\n",
    "        test_dataload = torch.load( 'data/%s/test_dataload_CasRNNGNN.pt'% data_name)\n",
    "        train_sets, valid_sets, test_sets = train_dataload.dataset, valid_dataload.dataset, test_dataload.dataset\n",
    "    train_dataload = DataLoader(train_sets, batch_size=512, shuffle=True,collate_fn=batch_padding, num_workers=0)\n",
    "    valid_dataload = DataLoader(valid_sets, batch_size=512, shuffle=False,collate_fn=batch_padding, num_workers=0)\n",
    "    test_dataload = DataLoader(test_sets, batch_size=512, shuffle=False,collate_fn=batch_padding, num_workers=0)\n",
    "    # data_name = 'Computers'\n",
    "    if data_name == 'Application':\n",
    "        item_vocab_size = 75159 + 1\n",
    "        max_position = 40\n",
    "    elif data_name == 'Computers':\n",
    "        item_vocab_size =  93140 + 1\n",
    "        max_position = 30\n",
    "    else:\n",
    "        item_vocab_size = 183561 + 1\n",
    "        max_position = 50\n",
    "    return train_dataload, valid_dataload, test_dataload, item_vocab_size, max_position"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from Model.EMBSR import EMBSR\n",
    "def train_process(train_data, model,criterion, opti, epoch):\n",
    "    losses = 0\n",
    "    steps = len(train_data)\n",
    "    # criterion = nn.CrossEntropyLoss().cuda()\n",
    "    for step, (x_items, x_alias, x_A, x_len, x_macro, x_micro, x_micro_len, x_action, x_pairs, x_poses, y_train)  in enumerate(train_data):\n",
    "        # new_index = [ i for i in range(1, x_poses.size(1))] + [0]\n",
    "        # x_action, x_poses, x_pairs = x_action[:,new_index], x_poses[:, new_index], x_pairs[:, new_index, new_index]\n",
    "        opti.zero_grad()\n",
    "        q = model(x_items.cuda(), x_A.float().cuda(), x_alias.cuda(), x_len.cuda(), x_macro.cuda(), x_micro.cuda(), x_micro_len.cuda(),x_action.long().cuda(),x_pairs.long().cuda(), x_poses.long().cuda())\n",
    "        target_items = y_train\n",
    "        # loss = model.CosSimilarityLoss(q, target_items.cuda())\n",
    "        loss = criterion(q, target_items.cuda()-1)\n",
    "        loss.backward()\n",
    "        opti.step()\n",
    "        losses += loss.item()\n",
    "        if (step + 1) % 100 == 0:\n",
    "            # 打印迭代轮次与训练时间\n",
    "            print(\"[%02d/%d] [%03d/%d] mean_loss : %0.2f\" % (epoch, 30, step, steps, losses / step + 1)) \n",
    "\n",
    "def valid_process(valid_data, model, valid_test):\n",
    "    y_pre_item_all = torch.LongTensor().cuda()\n",
    "    y_pre_item_all_5 = torch.LongTensor().cuda()\n",
    "    y_pre_item_all_10 = torch.LongTensor().cuda()\n",
    "    valid_target_item = torch.LongTensor().cuda()\n",
    "    for x_test_items, x_test_alias, x_test_A, x_test_len, x_test_macro, x_test_micro, x_test_micro_len, x_action_test, x_pairs_test, x_poses_test, y_test in valid_data:\n",
    "        with torch.no_grad():\n",
    "            # if valid_test == 'test':\n",
    "            #     x_action_test, x_pairs_test, x_poses_test = x_action_test[:,:-1], x_pairs_test[:, :-1, :-1], x_poses_test[:, :-1]\n",
    "            pre_items_5  = model(x_test_items.cuda(), x_test_A.float().cuda(), x_test_alias.cuda(), x_test_len.cuda(), x_test_macro.cuda(), x_test_micro.cuda(), x_test_micro_len.cuda(), x_action_test.long().cuda(), x_pairs_test.long().cuda(), x_poses_test.long().cuda()).topk(5, dim=1)[1]\n",
    "            pre_items_10  = model(x_test_items.cuda(), x_test_A.float().cuda(), x_test_alias.cuda(), x_test_len.cuda(), x_test_macro.cuda(), x_test_micro.cuda(), x_test_micro_len.cuda(), x_action_test.long().cuda(), x_pairs_test.long().cuda(), x_poses_test.long().cuda()).topk(10, dim=1)[1]\n",
    "            pre_items  = model(x_test_items.cuda(), x_test_A.float().cuda(), x_test_alias.cuda(), x_test_len.cuda(), x_test_macro.cuda(), x_test_micro.cuda(), x_test_micro_len.cuda(), x_action_test.long().cuda(), x_pairs_test.long().cuda(), x_poses_test.long().cuda()).topk(20, dim=1)[1]\n",
    "            y_pre_item_all_5 = torch.cat((y_pre_item_all_5, pre_items_5), 0)\n",
    "            y_pre_item_all_10 = torch.cat((y_pre_item_all_10, pre_items_10), 0)            \n",
    "            y_pre_item_all = torch.cat((y_pre_item_all, pre_items), 0)\n",
    "        valid_target_item = torch.cat((valid_target_item, y_test.cuda()))\n",
    "    items_recall_5 = get_recall(y_pre_item_all_5, valid_target_item.unsqueeze(1)-1)\n",
    "    items_mrr_5 = get_mrr(y_pre_item_all_5, valid_target_item.unsqueeze(1)-1)\n",
    "    items_ndcg_5 = get_ndcg(y_pre_item_all_5.cpu(), valid_target_item.unsqueeze(1).cpu()-1)\n",
    "    items_recall_10 = get_recall(y_pre_item_all_10, valid_target_item.unsqueeze(1)-1)\n",
    "    items_mrr_10 = get_mrr(y_pre_item_all_10, valid_target_item.unsqueeze(1)-1)\n",
    "    items_ndcg_10 = get_ndcg(y_pre_item_all_10.cpu(), valid_target_item.unsqueeze(1).cpu()-1)\n",
    "    items_recall = get_recall(y_pre_item_all, valid_target_item.unsqueeze(1)-1)\n",
    "    items_mrr = get_mrr(y_pre_item_all, valid_target_item.unsqueeze(1)-1)\n",
    "    items_ndcg = get_ndcg(y_pre_item_all.cpu(), valid_target_item.unsqueeze(1).cpu()-1)\n",
    "    print('%s Result:' % valid_test + \"H@20: \" + \"%.4f\" % items_recall + \"  MRR@20:\" + \"%.4f\" % items_mrr.tolist() + \" NDCG@20:\" + \"%.4f\" % items_ndcg.tolist() )\n",
    "    if valid_test == 'test':\n",
    "        print('%s Result:' % valid_test + \"H@5: \" + \"%.4f\" % items_recall_5 + \"  MRR@5:\" + \"%.4f\" % items_mrr_5.tolist() + \" NDCG@5:\" + \"%.4f\" % items_ndcg_5.tolist() )\n",
    "        print('%s Result:' % valid_test + \"H@10: \" + \"%.4f\" % items_recall_10 + \"  MRR@10:\" + \"%.4f\" % items_mrr_10.tolist() + \" NDCG@10:\" + \"%.4f\" % items_ndcg_10.tolist() )\n",
    "        return items_recall, items_mrr, [items_recall_5, items_mrr_5, items_ndcg_5,items_recall_10, items_mrr_10, items_ndcg_10,items_recall, items_mrr, items_ndcg]\n",
    "    else:\n",
    "        return items_recall, items_mrr, [items_recall, items_mrr, items_ndcg]\n",
    "\n",
    "def get_model(model_name, item_vocab_size, max_position, config, drop_, alpha_):\n",
    "    item_embedding_size, behavior_embedding_size, hidden_size = config[0], config[1], config[2]\n",
    "    if model_name == 'EMBSR':\n",
    "        model = EMBSR(item_vocab_size + 1, 11, max_position+1, 101, item_embedding_size, behavior_embedding_size, 100, hidden_size , drop_, alpha_)\n",
    "    elif model_name == 'EMBSR_trivago':\n",
    "        model = EMBSR(item_vocab_size + 1, 7, max_position+1, 50, item_embedding_size, behavior_embedding_size, 100, hidden_size , drop_, alpha_)\n",
    "    return model.cuda()\n",
    "\n",
    "\n",
    "def run(train_data, valid_data, test_data, item_vocab_size, max_position, model_name, config, result_file_name, lr_, drop_, alpha):                                                                                     \n",
    "    model =  get_model(model_name, item_vocab_size, max_position, config, drop_, alpha)\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    opti = optim.Adam(model.parameters(), lr=lr_, weight_decay=0, amsgrad=True)\n",
    "    best_hr, best_mrr = 0, 0\n",
    "    best_test_hr, best_test_mrr = 0, 0\n",
    "    best_test_list = []\n",
    "    stop_number = 0\n",
    "    result_file = open(result_file_name, 'a+')\n",
    "    best_epoch = 0\n",
    "    for epoch in range(50):\n",
    "        if stop_number > 10:\n",
    "            break\n",
    "        print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        model.train()\n",
    "        train_process(train_data, model, criterion, opti, epoch)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_hr, valid_mrr, valid_results = valid_process(valid_data, model, 'valid')\n",
    "            test_hr, test_mrr, test_results = valid_process(test_data, model, 'test')\n",
    "            if epoch == 0:\n",
    "                best_hr, best_mrr = valid_hr, valid_mrr\n",
    "            best_ = (valid_hr - best_hr)/best_hr + (valid_mrr - best_mrr)/best_mrr\n",
    "            if best_ > 0:\n",
    "                stop_number = 0\n",
    "                best_hr, best_mrr = valid_hr, valid_mrr\n",
    "                best_test_hr, best_test_mrr = test_hr, test_mrr\n",
    "                best_test_list = test_results\n",
    "                best_epoch = epoch\n",
    "                # torch.save(model.state_dict(), 'BestModel/best_%s_%s_TKDE.pth' % (model_name, file_name))\n",
    "            else:\n",
    "                stop_number += 1\n",
    "            print(\"best valid HR: \"+ \"%.4f\" % best_hr + \" Best valid MRR: \"+ \"%.4f\" % best_mrr.tolist())\n",
    "            print(\"best valid epoch: \"+ str(best_epoch))\n",
    "        result_file.writelines(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        result_file.writelines('epoch: %s ' %str(epoch) + 'best epoch: %s \\r\\n' % str(best_epoch ) )\n",
    "        result_file.writelines('valid Result: ' + \"Recall@20: \" + \"%.4f\" % valid_results[0] + \"  MRR@20:\" + \"%.4f\" % valid_results[1].tolist() + \" NDCG@20:\" + \"%.4f \\r\\n\" % valid_results[2].tolist())\n",
    "        result_file.writelines('test Result: '+ \"Recall@5: \" + \"%.4f\" % test_results[0] + \"  MRR@5:\" + \"%.4f\" % test_results[1].tolist() + \" NDCG@5:\" + \"%.4f \\r\\n\" % test_results[2].tolist() )\n",
    "        result_file.writelines('test Result: '+ \"Recall@10: \" + \"%.4f\" % test_results[3] + \"  MRR@10:\" + \"%.4f\" % test_results[4].tolist() + \" NDCG@10:\" + \"%.4f \\r\\n\" % test_results[5].tolist() )\n",
    "        result_file.writelines('test Result: '+ \"Recall@20: \" + \"%.4f\" % test_results[6] + \"  MRR@20:\" + \"%.4f\" % test_results[7].tolist() + \" NDCG@20:\" + \"%.4f \\r\\n\" % test_results[8].tolist() )\n",
    "        result_file.writelines(\"===================================================== \\r\\n\")\n",
    "        print(\"==================================\")\n",
    "    result_file.writelines('Best Valid HR@20: ' + \"%.4f\" % best_hr + \"  Best MRR@20:\" + \"%.4f\" % best_mrr.tolist())\n",
    "    result_file.writelines('Best Test HR@20: ' + \"%.4f\" % best_test_hr + \"  Best MRR@20:\" + \"%.4f\" % best_test_mrr.tolist())\n",
    "    result_file.writelines(str(best_test_list))\n",
    "    result_file.writelines('***********************************************************\\r\\n')\n",
    "    result_file.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "parameters = [ 100, 100, 100]\n",
    "\n",
    "file_names = ['Application', 'Computers', 'Trivago']\n",
    "model_names = ['EMBSR']\n",
    "\n",
    "for file_name in file_names:\n",
    "    model_name = 'EMBSR'\n",
    "    process_train, process_valid, process_test = 0, 0, 0\n",
    "    process_train, process_valid, process_test, item_vocab_size, max_position = preprocess_data(file_name)\n",
    "    alpha = 12\n",
    "    if file_name =='Application':\n",
    "        lr_ = 0.001\n",
    "        drop_ = 0.2\n",
    "    elif file_name == 'Computers':\n",
    "        lr_ = 0.003\n",
    "        drop_ = 0.1\n",
    "    else:\n",
    "        lr_ = 0.001\n",
    "        drop_ = 0.5\n",
    "        model_name = 'EMBSR_trivago'\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    result_file = 'ExperResult/BestModel/result_BestNew_%s_%s' % (model_name, file_name)\n",
    "    # process_train, process_valid, process_test, item_vocab_size, max_position = preprocess_data(file_name)\n",
    "    run(process_train, process_valid, process_test, item_vocab_size, max_position, model_name, parameters, result_file, lr_, drop_, alpha)\n",
    "    # process_train, process_valid, process_test = 0, 0, 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}